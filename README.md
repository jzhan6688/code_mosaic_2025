# Etsy Review Summarizer with Vertex AI

This repository contains two small utilities that work together to generate concise, policy-compliant summaries of Etsy listing reviews using Google Vertex AI Gemini **Batch Prediction** and then export the results to a CSV file.

---
## 1 . Repository layout

| File | Purpose |
|------|---------|
| `vertex_ai_generation.py` | • Reads a newline-delimited JSON file in **Google Cloud Storage**  
  • Converts each record into a Gemini chat request  
  • Stages the requests in **BigQuery**  
  • Submits a Vertex AI **Batch Prediction** job that writes its predictions back to BigQuery |
| `bq_to_csv.py` | • Queries a Batch-Prediction results table  
  • Parses the embedded request / response JSON  
  • Produces a CSV with columns **listing_id, buyer_reviews, summary** |
| `summaries.csv` | Example output generated by `bq_to_csv.py` |

---
## 2 . Environment setup

### Conda (recommended)
```bash
conda create -n vertexai_env \
  python=3.11 \
  pandas \
  tqdm \
  google-cloud-storage \
  google-cloud-bigquery \
  google-cloud-aiplatform \
  db-dtypes \
  pyarrow \
  -c conda-forge

conda activate vertexai_env
# Vertex GenAI SDK is not yet on conda-forge ↴
pip install vertexai
```

### Pure-pip alternative
```bash
python -m venv venv
source venv/bin/activate
pip install pandas tqdm google-cloud-storage google-cloud-bigquery google-cloud-aiplatform db-dtypes pyarrow vertexai
```

> **☞ Authentication**  
> Make sure your local environment has application-default credentials:
> ```bash
> gcloud auth login               # open browser
> gcloud auth application-default login
> ```

---
## 3 . `vertex_ai_generation.py` – create the batch job

1. Upload your NDJSON file to GCS. Each line must look like one of these:
   ```jsonc
   { "listing_id": "123", "reviews": ["Great", "Loved it"] }
   // OR (if you already have an `input` wrapper)
   { "input": { "listing_id": "123", "reviews": ["Great", "Loved it"] } }
   ```
2. Adjust the constants at the top of the script if you need a different project, region, dataset, etc.
3. Run the script (replace the URI with your own):
   ```bash
   python vertex_ai_generation.py \
     --gcs-uri gs://my-bucket/listings.jsonl
   ```
   The script will:
   * create / overwrite a BigQuery staging table (`${PROJECT}.${DATASET}.${INPUT_TAB}`),
   * insert one row per listing, and
   * fire off the Gemini batch job.  
     The console output prints the job resource name so you can track progress:
     ```
     Job submitted: projects/…/batchPredictions/1234567890
     ```

4. Wait for the job to finish (few minutes to ~hour depending on size).
   ```bash
   gcloud ai batch-predictions describe 1234567890 | grep state
   ```

The predictions land in a BigQuery table named `predictions_<timestamp>_<id>` inside the same dataset.

---
## 4 . `bq_to_csv.py` – export the summaries

Once the batch job is **SUCCEEDED**, grab the fully-qualified table name (you can copy it from BigQuery UI) and run:
```bash
python bq_to_csv.py \
  --table etsy-inventory-ml-dev.iml_development.predictions_2025-05-13-22-03-26-9bd08 \
  --output summaries.csv
```
Optional flags:
```bash
# Only process first 100 rows while testing
python bq_to_csv.py --table … --output sample.csv --limit 100
```
The resulting CSV will have three columns:
```
listing_id,buyer_reviews,summary
253244255,"Very happy with my purchase | Beautiful. | Awesome, thanks! | These are real beauties…","Not enough data"
…
```

---
## 5 . Troubleshooting

• **ValueError: Please install the 'db-dtypes' package** – install `db-dtypes` and `pyarrow` (see env section).  
• **PERMISSION_DENIED** when querying BigQuery – ensure your active credentials have BigQuery read access.  
• **Vertex SDK import errors** – confirm `pip install vertexai` ran inside the active environment.